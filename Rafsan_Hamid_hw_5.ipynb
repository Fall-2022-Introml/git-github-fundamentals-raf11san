{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1b8e4c05",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2b7486560c5af484e3af5a24fcc058a4",
          "grade": false,
          "grade_id": "instructions",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1b8e4c05"
      },
      "source": [
        "#### Homework-5\n",
        "##### Total number of points: 70\n",
        "#### Due date: October 20th, 2022\n",
        "\n",
        "Before you submit this homework, make sure everything runs as expected. First, restart the kernel (in the menu, select Kernel → Restart) and then run all cells (in the menubar, select Cell → Run All). You can discuss with others regarding the homework but all work must be your own.\n",
        "\n",
        "This homework will test your knowledge on basics of Python. The Python notebooks shared will be helpful to solve these problems. \n",
        "\n",
        "Steps to evaluate your solutions:\n",
        "\n",
        "Step-1: Ensure you have installed Anaconda (Windows: https://docs.anaconda.com/anaconda/install/windows/ ; Mac:https://docs.anaconda.com/anaconda/install/mac-os/ ; Linux: https://docs.anaconda.com/anaconda/install/linux/)\n",
        "\n",
        "Step-2: Open the Jupyter Notebook by first launching the anaconda software console\n",
        "\n",
        "Step-3: Open the .ipynb file and write your solutions at the appropriate location \"# YOUR CODE HERE\"\n",
        "\n",
        "Step-4: You can restart the kernel and click run all (in the menubar, select Cell → Run All) on the center-right on the top of this window.\n",
        "\n",
        "Step-5: Now go to \"File\" then click on \"Download as\" then click on \"Notebook (.ipynb)\" Please DO NOT change the file name and just keep it as .ipynb file format\n",
        "\n",
        "Step-6: Go to lms.rpi.edu and upload your homework at the appropriate link to submit this homework.\n",
        "\n",
        "#### Please note that for any question in this assignment you will receive points ONLY if your solution passes all the test cases including hidden testcases as well. So please make sure you try to think all possible scenarios before submitting your answers.  \n",
        "- Note that hidden tests are present to ensure you are not hardcoding. \n",
        "- If caught cheating: \n",
        "    - you will receive a score of 0 for the 1st violation. \n",
        "    - for repeated incidents, you will receive an automatic 'F' grade and will be reported to the dean of Lally School of Management. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58506fd2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f47c5b997378dd9d94f3595001dcc828",
          "grade": false,
          "grade_id": "q1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "58506fd2"
      },
      "source": [
        "#### Q1 [10 points]. Please make sure this is correct as the following questions are dependent on this. \n",
        "- Load the boston data from sklearn library (library is already provided here below). \n",
        "##### Part-1:\n",
        "- The given target labels (boston.target) are continuous -- convert them into discrete values -- 1 and 2 using this approach and save them in an array `y_true`.\n",
        "- all the values (v1) in boston.target should become 1 if v1>=5 and v1<23; \n",
        "- all the values (v2) in boston.target should become 2 if v2>=23 and v2<51;\n",
        "- Note that these new/transformed labels 1 and 2 are **integers**\n",
        "- Please save these new discrete values in the array `y_true`\n",
        "\n",
        "##### Part-2: \n",
        "- Create a list `y_pred` which is of the same length as `y_true` and insert all values as 1.\n",
        "- Now use `y_true` and `y_pred` to calculate: 1) accuracy and assign it to variable `tacc`; 2) precision and assign it to variable `tprec`; 3) recall and assign it to variable `trecall`.\n",
        "- Create another list `y_pred_prob` which is of the same length as `y_true` and insert all values as 0.75. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "ba6285a9",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e0e47f6b96090ab0153de62181b2a716",
          "grade": false,
          "grade_id": "q1-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ba6285a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86cf9c8-170b-47a1-f87c-01d24cadab36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "#Load the dataset and save it as a dataframe\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "\n",
        "# Assigning y_true as a list\n",
        "y_true = []\n",
        "\n",
        "# Assigning all the v1 and v2 values and inserting them into y_true\n",
        "bins = np.array([5.0, 23.0, 51.0])\n",
        "y_true = np.digitize(boston.target, bins)\n",
        "\n",
        "# Assigning 506 1s into y_pred\n",
        "y_pred = [1] * len(y_true)\n",
        "\n",
        "# Calculating accuracy, precision, recall, and roc/auc\n",
        "from sklearn import metrics\n",
        "tacc = metrics.accuracy_score(y_true, y_pred)\n",
        "tprec = metrics.precision_score(y_true, y_pred)\n",
        "trecall = metrics.recall_score(y_true, y_pred)\n",
        "tauc = metrics.roc_auc_score(y_true, y_pred)\n",
        "\n",
        "# Assigning 506 0.75s into y_pred_prob\n",
        "y_pred_prob = [0.75] * len(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "ac3f11f8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3522fb66fff2f12553f2c9174d55c19a",
          "grade": true,
          "grade_id": "1",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ac3f11f8"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-1 \n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert set(y_true)=={1,2}\n",
        "assert (list(y_true)).count(1)==312\n",
        "assert (list(y_true)).count(0)==0\n",
        "assert (list(y_true)).count(2)==194"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "neither-discussion",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f3852e85799ed282f1634971ae985a79",
          "grade": true,
          "grade_id": "2",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "neither-discussion"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-2\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert round(tacc, 2)==0.62\n",
        "assert round(tprec, 2)==0.62\n",
        "assert round(trecall, 2)==1.0\n",
        "assert round(tauc, 2)==0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fea31d1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bc05c36c4783af238926586ee83dff3a",
          "grade": false,
          "grade_id": "q2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5fea31d1"
      },
      "source": [
        "#### Q2 [5 points]. Now split the data into training (80%) and testing data (20%) by using these variable names \n",
        "- `X_train`: Training feature columns\n",
        "- `X_test`: Testing feature columns\n",
        "- `y_train`: Training labels\n",
        "- `y_test`: Testing labels\n",
        "- with only parameters df, y_true and 'test_size'. `df` and `y_true` are initialized in the previous question. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ec16ae9d",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "296509c5901df0967f2639328a908f79",
          "grade": false,
          "grade_id": "q2-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ec16ae9d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train = pd.DataFrame()\n",
        "y_train = pd.DataFrame()\n",
        "X_test = pd.DataFrame()\n",
        "y_test = pd.DataFrame()\n",
        "\n",
        "# Splitting the data into 80% training and 20% testing \n",
        "X_train, X_test, y_train, y_test = train_test_split(df, y_true, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "2dc94621",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7807594e654d0c85c4dd36c803324291",
          "grade": true,
          "grade_id": "3",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2dc94621"
      },
      "outputs": [],
      "source": [
        "#[10 points] Test cell-3\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert len(X_train)==404\n",
        "assert len(y_test)==102\n",
        "assert len(y_train)==404\n",
        "assert len(X_test)==102"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f080f1b9",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "28d349a4ffec8343d2d59fd6684a0393",
          "grade": false,
          "grade_id": "q3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "f080f1b9"
      },
      "source": [
        "#### Q3 [5 points]. Use the `df` in Q1 to perform these operations: \n",
        "<ol>\n",
        "<li> Standardize the data using the StandardScaler() function from sklearn library</li>\n",
        "<li> Compute principal components using the fit_transform operation on the original dataset </li>\n",
        "<li> Now find the number of principal components `n_components` that will retain not more than 75% of the information present in the original dataset </li>\n",
        "</ol>\n",
        "\n",
        "- Make sure you declare appropriate Python packages which are not provided by default\n",
        "- Hint -- Use explained_variance_ratio_.cumsum() function we discussed in the class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "a4a4c265",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b1b968f5bf118d91cdc4e042a8c4c28b",
          "grade": false,
          "grade_id": "q3-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "a4a4c265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2233fb8-fc75-47ee-bcc4-1f38b7ec01f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Load the dataset and save it as a dataframe\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "\n",
        "n_components=0\n",
        "\n",
        "# Standardizing data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Computing PCA\n",
        "pca = PCA()\n",
        "scaled_data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Finding the number of principal components \n",
        "scaled_data_pca_75 = pca.fit(scaled_data)\n",
        "pca_cumsum = scaled_data_pca_75.explained_variance_ratio_.cumsum()\n",
        "n_components = len(np.where(pca_cumsum <= 0.75)[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "7980f1c4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0c20d1521e4ef426344f79c6baff3c0a",
          "grade": true,
          "grade_id": "4",
          "locked": true,
          "points": 2.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7980f1c4"
      },
      "outputs": [],
      "source": [
        "#[2.5 points] Test cell-4\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert n_components!=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "954bda19",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f9d2701500e2802390db995dcbc38a3d",
          "grade": true,
          "grade_id": "5",
          "locked": true,
          "points": 2.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "954bda19"
      },
      "outputs": [],
      "source": [
        "#[2.5 points] Test cell-5 Hidden tests\n",
        "#DO NOT MODIFY/DELETE THIS CELL "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b47c63a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "89db1c6a505cffbf15688bd8cde7444a",
          "grade": false,
          "grade_id": "q4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7b47c63a"
      },
      "source": [
        "#### Q4 [10 points]. Given 3 different series `S1`, `S2`, `S3`\n",
        "<ol>\n",
        "<li>First, using these 3 series build a dataframe 'df' where the column name associated with each series is same as the name of the series such as `S1`, `S2` or `S3`.</li>\n",
        "<li>Please note that 'df' doesn't contain any class label column. We are building a simple dataframe with 3 columns from 3 different series each considered as a feature.</li>\n",
        "<li>If you perform PCA operation, how many maximum principal components will you obtain?</li>\n",
        "<li>No need to do pca but manually assign the value to an integer variable `max_pcs`</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "ae2308ca",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e3b4bac3ff8122fe6b11572be6df21ff",
          "grade": false,
          "grade_id": "q4-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ae2308ca"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "S1 = pd.Series([1,2,3,4]) #corresponding column name should be 'S1'\n",
        "S2 = pd.Series([10, 20, 30, 40]) #corresponding column name should be 'S2'\n",
        "S3 = pd.Series([2,4,6,8]) #corresponding column name should be 'S3'\n",
        "\n",
        "max_pcs=0 #Update this variable after building the dataframe #No need to perform PCA \n",
        "\n",
        "# Creating the dataframe\n",
        "df = pd.DataFrame({'S1': S1, 'S2': S2, 'S3': S3})\n",
        "\n",
        "# Scaling data\n",
        "scaler = MinMaxScaler()\n",
        "df_rescaled = scaler.fit_transform(df)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit_transform(df_rescaled)\n",
        "\n",
        "# Assigning maximum principal components into max_pcs\n",
        "max_pcs = pca.n_components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "6a6c71e7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "21b2b7eba8c74e1567bf167ab1379b73",
          "grade": true,
          "grade_id": "6",
          "locked": true,
          "points": 7.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "6a6c71e7"
      },
      "outputs": [],
      "source": [
        "#[7.5 points] Test cell-6\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert set(df.columns)=={'S1', 'S2', 'S3'}\n",
        "assert df['S3'].mean()==5.0\n",
        "assert df['S1'].mean()==2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "10ff071a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d4e5c6493cf5708aca54155ce02f61f0",
          "grade": true,
          "grade_id": "7",
          "locked": true,
          "points": 2.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "10ff071a"
      },
      "outputs": [],
      "source": [
        "#[2.5 points] Test cell-7 Hidden tests\n",
        "#DO NOT MODIFY/DELETE THIS CELL "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0428eb49",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "33299971990a86d1865c0bfc29c63757",
          "grade": false,
          "grade_id": "q5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0428eb49"
      },
      "source": [
        "#### Q5 [10 points]. Linear regression: Use the boston dataset loaded into the dataframe `df`\n",
        "##### 1. **Without using train_test_split() function**\n",
        "- use the 1st 400 rows (i.e., index 0,1,....399) where the corresponding features are loaded as `train_X` and corresponding labels as `train_y`\n",
        "- Use the remaining rows as testing data -- `test_X` and `test_y` for the features and labels respectively.\n",
        "- Fit a linear regression line using the training data; then use it to predict labels for testing data as shown in the lecture notebook. Please use the default parameters when calling the linear regression function. \n",
        "\n",
        "##### 2. Measure the mean-squared error (MSE) 'mse_split1' using the predicted labels with testing labels.\n",
        "- Round the `mse_split1` to 2 values after the decimal point. \n",
        "- Hint -- Check out the libraries given to guess which functions to use to compute MSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "8ac2a6e6",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a55cbe99214cd58689c82add6c0d3031",
          "grade": false,
          "grade_id": "q5-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8ac2a6e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b1ff27-5bfa-4a28-a0b8-dc5601d31e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "#Load the dataset without class labels and save it as a dataframe\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "#class label -- boston.target\n",
        "\n",
        "train_X = pd.DataFrame()\n",
        "train_y = pd.DataFrame()\n",
        "test_X = pd.DataFrame()\n",
        "test_y = pd.DataFrame()\n",
        "\n",
        "mse_split1=0\n",
        "\n",
        "# Assigning train_X and train_y\n",
        "train_X = df.iloc[:400, :]\n",
        "train_y = boston.target[:400]\n",
        "\n",
        "# Assigning test_X and test_y\n",
        "test_X = df.iloc[400:, :]\n",
        "test_y = boston.target[400:]\n",
        "\n",
        "# Linear regression\n",
        "data_regression = linear_model.LinearRegression()\n",
        "data_regression.fit(train_X, train_y)\n",
        "pred = data_regression.predict(test_X)\n",
        "\n",
        "# Measuring MSE\n",
        "mse_split1 = round(mean_squared_error(test_y, pred), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "4fad5071",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2a9150696e2bc13f6886acc6a1409884",
          "grade": true,
          "grade_id": "8",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4fad5071"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-8\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert len(train_X)==400\n",
        "assert len(test_y)==106\n",
        "assert mse_split1==37.89\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "8ce17d97",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e3f60ce67cda230ee5e51a54babc4f39",
          "grade": true,
          "grade_id": "9",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8ce17d97"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-9 Hidden tests \n",
        "#DO NOT MODIFY/DELETE THIS CELL "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0599097",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e8d2e784ec626eaf2b4571b5f17f79c1",
          "grade": false,
          "grade_id": "q6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "b0599097"
      },
      "source": [
        "#### Q6 [10 points]. Repeat the above exercise (Q5) but with different training and testing splits using the `boston` dataset.\n",
        "\n",
        "###### 1. **Without using train_test_split() function** \n",
        "- use the last set of rows from index 253 to the end (i.e., index 253,254,....505) where, the corresponding features are loaded as `train_X` and corresponding labels as `train_y`\n",
        "- Use the remaining rows (from index 0, 1, 2, ...., 252) as testing data -- `test_X` and `test_y` for the features and labels respectively.\n",
        "- Fit a linear regression line using the training data; then use it for predicting testing data as shown in the lecture notebook. Please use the default parameters when calling the linear regression function. \n",
        "\n",
        "###### 2. Measure the mean-squared error (MSE) 'mse_split2' using the predicted labels with testing labels.\n",
        "- Round the `mse_split2` to 2 values after the decimal point. \n",
        "- Hint -- Check out the libraries given to guess which functions to use to compute MSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "2dcd008d",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9a83bf208a4218d9752e7246175da5d1",
          "grade": false,
          "grade_id": "q6-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "2dcd008d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51373e2-a9e2-4b8f-bcc3-65131a4d5fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Load the dataset without class labels and save it as a dataframe\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "#class label -- boston.target\n",
        "\n",
        "train_X = pd.DataFrame()\n",
        "train_y = pd.DataFrame()\n",
        "test_X = pd.DataFrame()\n",
        "test_y = pd.DataFrame()\n",
        "\n",
        "mse_split2=0\n",
        "\n",
        "# Assigning train_X and train_y\n",
        "train_X = df.iloc[253:, :]\n",
        "train_y = boston.target[253:]\n",
        "\n",
        "# Assigning test_X and test_y\n",
        "test_X = df.iloc[:253, :]\n",
        "test_y = boston.target[:253]\n",
        "\n",
        "# Linear regression\n",
        "data_regression2 = linear_model.LinearRegression()\n",
        "data_regression2.fit(train_X, train_y)\n",
        "pred2 = data_regression2.predict(test_X)\n",
        "\n",
        "# Measuring MSE\n",
        "mse_split2 = round(mean_squared_error(test_y, pred2), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "1c3240fe",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cfaa786287ef65f5279855628ef0d06c",
          "grade": true,
          "grade_id": "10",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1c3240fe"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-10\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert len(train_X)==253\n",
        "assert len(test_y)==253\n",
        "assert mse_split2==27.22\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "29e3b1f6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d57defad1a2ab37402a3251770148a6e",
          "grade": true,
          "grade_id": "11",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "29e3b1f6"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-11 Hidden tests \n",
        "#DO NOT MODIFY/DELETE THIS CELL "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "899af61b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6054edc8cde84020c55bc44115f25716",
          "grade": false,
          "grade_id": "q7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "899af61b"
      },
      "source": [
        "#### Q7 [15 points]. We have loaded the boston dataset -- use the dataframe `df` to do: \n",
        "###### 1. Convert the target labels -- boston.target from continuous to discrete values -- 1, 2 and 3 using this approach.\n",
        "- all the values (lets say, each value is represented by v1) in boston.target should become 1 if v1>=5 and v1<20; \n",
        "- all the values (lets say, each value is represented by v2) in boston.target should become 2 if v2>=20 and v2<35;\n",
        "- all the values (lets say, each value is represented by v3) in boston.target should become 3 if v3>=35 and v3<51;\n",
        "\n",
        "###### 2. **Without using train_test_split()** use the 1st 400 rows (i.e., index 0,1,....399) where, \n",
        "- the corresponding features are loaded as `train_X` and corresponding labels as `train_y`\n",
        "- Use the rest of the rows for testing data -- `test_X` and `test_y` for the features and groundtruth labels respectively.\n",
        "- Fit a logistic regression with solver=`newton-cg`, C=`1e5`, multi_class=`multinomial`; then use it for predicting testing data as shown in the lecture notebook. If you encounter this warning, please ignore it -- \"ConvergenceWarning: newton-cg failed to converge.\" \n",
        "\n",
        "###### 3. Measure the accuracy `acc_split3` using the predicted labels with groundtruth test labels `test_y`.\n",
        "- Round the `acc_split3` to 2 values after the decimal point. \n",
        "- Hint -- Check out the libraries given to guess which functions to use to compute accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "5b2c5c66",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e6cd1dfdd9a2cea610553e387bf80665",
          "grade": false,
          "grade_id": "q7-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5b2c5c66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c71a9a-9071-4070-893f-ee3f41306f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Load the dataset without class labels and save it as a dataframe\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "#class label -- boston.target\n",
        "\n",
        "train_X = pd.DataFrame()\n",
        "train_y = pd.DataFrame()\n",
        "test_X = pd.DataFrame()\n",
        "test_y = pd.DataFrame()\n",
        "\n",
        "acc_split3=0\n",
        "\n",
        "# Assigning all the v1, v2, and v3 values\n",
        "bins = np.array([5.0, 20.0, 35.0, 51.0])\n",
        "target_labels = np.digitize(boston.target, bins)\n",
        "\n",
        "# Assigning train_X and train_y\n",
        "train_X = df.iloc[:400, :]\n",
        "train_y = target_labels[:400]\n",
        "\n",
        "# Assigning test_X and test_y\n",
        "test_X = df.iloc[400:, :]\n",
        "test_y = target_labels[400:]\n",
        "\n",
        "# Logistic regression\n",
        "data_regression3 = linear_model.LogisticRegression(solver = 'newton-cg', C = 1e5, multi_class = 'multinomial')\n",
        "data_regression3.fit(train_X, train_y)\n",
        "pred2 = data_regression3.predict(test_X)\n",
        "\n",
        "# Measuring accuracy\n",
        "acc_split3 = round(accuracy_score(test_y, pred2), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "16101e15",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "126edaf051cdd05f64d5374b999b65f3",
          "grade": true,
          "grade_id": "12",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "16101e15"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-12\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert len(train_X)==400\n",
        "assert set(train_y)=={1,2,3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "319e41c2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8876b466ba89c7821744b0310d0823d1",
          "grade": true,
          "grade_id": "13",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "319e41c2"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-13\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert acc_split3==0.74\n",
        "assert len(test_y)==106"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "6318191b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5d28da0763c4891098b4f6f4a5c695d7",
          "grade": true,
          "grade_id": "14",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "6318191b"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-14 Hidden tests \n",
        "#DO NOT MODIFY/DELETE THIS CELL "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}